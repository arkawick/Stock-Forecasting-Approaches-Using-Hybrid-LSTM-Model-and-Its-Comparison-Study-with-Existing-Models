{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting index as date values\n",
    "df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')\n",
    "df.index = df['Date']\n",
    "\n",
    "#sorting\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "\n",
    "#creating a separate dataset\n",
    "new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    new_data['Date'][i] = data['Date'][i]\n",
    "    new_data['Close'][i] = data['Close'][i]\n",
    "    \n",
    "#create features\n",
    "from fastai.structured import  add_datepart\n",
    "add_datepart(new_data, 'Date')\n",
    "new_data.drop('Elapsed', axis=1, inplace=True)  #elapsed will be the time stamp\n",
    "\n",
    "new_data['mon_fri'] = 0\n",
    "for i in range(0,len(new_data)):\n",
    "    if (new_data['Dayofweek'][i] == 0 or new_data['Dayofweek'][i] == 4):\n",
    "        new_data['mon_fri'][i] = 1\n",
    "    else:\n",
    "        new_data['mon_fri'][i] = 0\n",
    "\n",
    "#split into train and validation\n",
    "train = new_data[:987]\n",
    "valid = new_data[987:]\n",
    "\n",
    "x_train = train.drop('Close', axis=1)\n",
    "y_train = train['Close']\n",
    "x_valid = valid.drop('Close', axis=1)\n",
    "y_valid = valid['Close']\n",
    "\n",
    "#implement linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions and find the rmse\n",
    "preds = model.predict(x_valid)\n",
    "rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))\n",
    "rms\n",
    "121.16291596523156\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "sp500data = yf.download(\"^GSPC\", start=\"2017-01-01\", end=\"2021-10-16\")\n",
    "sp500_df = pd.DataFrame(sp500_data)\n",
    "sp500_df.to_csv(\"sp500_data.csv\")\n",
    "\n",
    "\n",
    "read_df = pd.read_csv(\"sp500_data.csv\")\n",
    "read_df.set_index(\"Date\", inplace=True)\n",
    "read_df['Adj Close'].plot()\n",
    "plt.ylabel(\"Adjusted Close Prices\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"sp500_data.csv\")\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "x = df.iloc[:, 0:5].values\n",
    "y = df.iloc[:, 4].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.26,  random_state=0)\n",
    "\n",
    "scale = StandardScaler()\n",
    "x_train = scale.fit_transform(x_train)\n",
    "x_test = scale.transform(x_test)\n",
    "\n",
    "\n",
    "grid_rf = {\n",
    "'n_estimators': [20, 50, 100, 500, 1000],  \n",
    "'max_depth': np.arange(1, 15, 1),  \n",
    "'min_samples_split': [2, 10, 9], \n",
    "'min_samples_leaf': np.arange(1, 15, 2, dtype=int),  \n",
    "'bootstrap': [True, False], \n",
    "'random_state': [1, 2, 30, 42]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "rscv = RandomizedSearchCV(estimator=model, param_distributions=grid_rf, cv=3, n_jobs=-1, verbose=2, n_iter=200)\n",
    "rscv_fit = rscv.fit(x_train, y_train)\n",
    "best_parameters = rscv_fit.best_params_\n",
    "print(best_parameters)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=500, random_state=42, min_samples_split=2, \n",
    "                              min_samples_leaf=1, max_depth=10, bootstrap=True)\n",
    "model.fit(x_train, y_train)\n",
    "predict = model.predict(x_test)\n",
    "print(predict)\n",
    "print(predict.shape)\n",
    "\n",
    "\n",
    "print(\"Mean Absolute Error:\", round(metrics.mean_absolute_error(y_test, predict), 4))\n",
    "print(\"Mean Squared Error:\", round(metrics.mean_squared_error(y_test, predict), 4))\n",
    "print(\"Root Mean Squared Error:\", round(np.sqrt(metrics.mean_squared_error(y_test, predict)), 4))\n",
    "print(\"(R^2) Score:\", round(metrics.r2_score(y_test, predict), 4))\n",
    "print(f'Train Score : {model.score(x_train, y_train) * 100:.2f}% and Test Score : {model.score(x_test, y_test) * 100:.2f}% using Random Tree Regressor.')\n",
    "errors = abs(predict - y_test)\n",
    "mape = 100 * (errors / y_test)\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result:\n",
    "Mean Absolute Error: 2.2478\n",
    "Mean Squared Error: 15.9311\n",
    "Root Mean Squared Error: 3.9914\n",
    "(R^2) Score: 1.0\n",
    "Train Score : 100.00% and Test Score : 100.00% using Random Tree Regressor.\n",
    "Accuracy: 99.93 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Using the same train and validation set from the last section:\n",
    "\n",
    "#scaling data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train_scaled)\n",
    "x_valid_scaled = scaler.fit_transform(x_valid)\n",
    "x_valid = pd.DataFrame(x_valid_scaled)\n",
    "\n",
    "#using gridsearch to find the best parameter\n",
    "params = {'n_neighbors':[2,3,4,5,6,7,8,9]}\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "model = GridSearchCV(knn, params, cv=5)\n",
    "\n",
    "#fit the model and make predictions\n",
    "model.fit(x_train,y_train)\n",
    "preds = model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse\n",
    "rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))\n",
    "rms\n",
    "115.17086550026721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyramid.arima import auto_arima\n",
    "\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "\n",
    "train = data[:987]\n",
    "valid = data[987:]\n",
    "\n",
    "training = train['Close']\n",
    "validation = valid['Close']\n",
    "\n",
    "model = auto_arima(training, start_p=1, start_q=1,max_p=3, max_q=3, \n",
    "                   m=12,start_P=0, seasonal=True,d=1, D=1, trace=True,\n",
    "                   error_action='ignore',suppress_warnings=True)\n",
    "model.fit(training)\n",
    "\n",
    "forecast = model.predict(n_periods=248)\n",
    "forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results\n",
    "rms=np.sqrt(np.mean(np.power((np.array(valid['Close'])-np.array(forecast['Prediction'])),2)))\n",
    "rms\n",
    "44.954584993246954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet -- fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing prophet\n",
    "from fbprophet import Prophet\n",
    "\n",
    "#creating dataframe\n",
    "new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    new_data['Date'][i] = data['Date'][i]\n",
    "    new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "new_data['Date'] = pd.to_datetime(new_data.Date,format='%Y-%m-%d')\n",
    "new_data.index = new_data['Date']\n",
    "\n",
    "#preparing data\n",
    "new_data.rename(columns={'Close': 'y', 'Date': 'ds'}, inplace=True)\n",
    "\n",
    "#train and validation\n",
    "train = new_data[:987]\n",
    "valid = new_data[987:]\n",
    "\n",
    "#fit the model\n",
    "model = Prophet()\n",
    "model.fit(train)\n",
    "\n",
    "#predictions\n",
    "close_prices = model.make_future_dataframe(periods=len(valid))\n",
    "forecast = model.predict(close_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results\n",
    "#rmse\n",
    "forecast_valid = forecast['yhat'][987:]\n",
    "rms=np.sqrt(np.mean(np.power((np.array(valid['y'])-np.array(forecast_valid)),2)))\n",
    "rms\n",
    "57.494461930575149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "#creating dataframe\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "for i in range(0,len(data)):\n",
    "    new_data['Date'][i] = data['Date'][i]\n",
    "    new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "#setting index\n",
    "new_data.index = new_data.Date\n",
    "new_data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "#creating train and test sets\n",
    "dataset = new_data.values\n",
    "\n",
    "train = dataset[0:987,:]\n",
    "valid = dataset[987:,:]\n",
    "\n",
    "#converting dataset into x_train and y_train\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "x_train, y_train = [], []\n",
    "for i in range(60,len(train)):\n",
    "    x_train.append(scaled_data[i-60:i,0])\n",
    "    y_train.append(scaled_data[i,0])\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)\n",
    "\n",
    "#predicting 246 values, using past 60 from the train data\n",
    "inputs = new_data[len(new_data) - len(valid) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs  = scaler.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "for i in range(60,inputs.shape[0]):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "closing_price = model.predict(X_test)\n",
    "closing_price = scaler.inverse_transform(closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results\n",
    "rms=np.sqrt(np.mean(np.power((valid-closing_price),2)))\n",
    "rms\n",
    "11.772259608962642"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Average\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "\n",
    "# define inputlayer\n",
    "InputLayer = Input(shape=(len_input,40))\n",
    "\n",
    "# define cnn\n",
    "Conv1 = Conv1D(filters=64, kernel_size=3, activation='tanh', input_shape=(len_input,40))(InputLayer)\n",
    "Conv2 = Conv1D(filters=64, kernel_size=3, activation='tanh', input_shape=(len_input,40))(Conv1)\n",
    "Pool1 = MaxPooling1D(pool_size=5)(Conv2)\n",
    "Conv3 = Conv1D(filters=128, kernel_size=3, activation='tanh', input_shape=(len_input,40))(Pool1)\n",
    "Conv4 = Conv1D(filters=128, kernel_size=3, activation='tanh', input_shape=(len_input,40))(Conv3)\n",
    "Pool2 = MaxPooling1D(pool_size=5)(Conv4)\n",
    "Conv5 = Conv1D(filters=256, kernel_size=3, activation='tanh', input_shape=(len_input,40))(Pool2)\n",
    "Conv6 = Conv1D(filters=256, kernel_size=3, activation='tanh', input_shape=(len_input,40))(Conv5)\n",
    "Pool3 = MaxPooling1D(pool_size=5)(Conv6)\n",
    "Conv7 = Conv1D(filters=512, kernel_size=3, activation='tanh', input_shape=(len_input,40))(Pool3)\n",
    "Conv8 = Conv1D(filters=512, kernel_size=3, activation='tanh', input_shape=(len_input,40))(Conv7)\n",
    "Pool4 = MaxPooling1D(pool_size=5)(Conv8)\n",
    "Flat = Flatten()(Pool4)\n",
    "Dense1 = Dense(512, activation='tanh')(Flat)\n",
    "Dense2 = Dense(256)(Dense1)\n",
    "Repeater1 = RepeatVector(600)(Dense2)\n",
    "\n",
    "# define first lstm\n",
    "# Shape = Reshape((1, len_input, 40)(InputLayer)\n",
    "LSTM1 = LSTM(256, activation='tanh', input_shape=(len_input,40), return_sequences=False)(InputLayer)\n",
    "Repeater2 = RepeatVector(600)(LSTM1)\n",
    "\n",
    "# define the combined output\n",
    "Conc = Average()([Repeater1, Repeater2])\n",
    "\n",
    "# define second lstm\n",
    "LSTM2 = LSTM(256, activation='tanh', return_sequences=True)(Conc)\n",
    "TimeDistr = TimeDistributed(Dense(1))(LSTM2)\n",
    "\n",
    "Model = Model(inputs=InputLayer, outputs=TimeDistr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating model and training\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import MaxPooling1D, Flatten\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Creating the Neural Network model here...\n",
    "# CNN layers\n",
    "model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu', input_shape=(None, 100, 1))))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Conv1D(128, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Conv1D(64, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling1D(2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# model.add(Dense(5, kernel_regularizer=L2(0.01)))\n",
    "\n",
    "# LSTM layers\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Final layers\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "\n",
    "history = model.fit(train_X, train_Y, validation_data=(test_X,test_Y), epochs=40,batch_size=40, verbose=1, shuffle =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis\n",
    "from pmdarima import auto_arima\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from talib import abstract\n",
    "import json\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(actual, prediction):\n",
    "    actual = pd.Series(actual)\n",
    "    prediction = pd.Series(prediction)\n",
    "    return 100 * np.mean(np.abs((actual - prediction))/actual)\n",
    "\n",
    "\n",
    "def get_arima(data, train_len, test_len):\n",
    "    # prepare train and test data\n",
    "    data = data.tail(test_len + train_len).reset_index(drop=True)\n",
    "    train = data.head(train_len).values.tolist()\n",
    "    test = data.tail(test_len).values.tolist()\n",
    "\n",
    "    # Initialize model\n",
    "    model = auto_arima(train, max_p=3, max_q=3, seasonal=False, trace=True,\n",
    "                       error_action='ignore', suppress_warnings=True)\n",
    "\n",
    "    # Determine model parameters\n",
    "    model.fit(train)\n",
    "    order = model.get_params()['order']\n",
    "    print('ARIMA order:', order, '\\n')\n",
    "\n",
    "    # Genereate predictions\n",
    "    prediction = []\n",
    "    for i in range(len(test)):\n",
    "        model = pm.ARIMA(order=order)\n",
    "        model.fit(train)\n",
    "        print('working on', i+1, 'of', test_len, '-- ' + str(int(100 * (i + 1) / test_len)) + '% complete')\n",
    "        prediction.append(model.predict()[0])\n",
    "        train.append(test[i])\n",
    "\n",
    "    # Generate error data\n",
    "    mse = mean_squared_error(test, prediction)\n",
    "    rmse = mse ** 0.5\n",
    "    mape = mean_absolute_percentage_error(pd.Series(test), pd.Series(prediction))\n",
    "    return prediction, mse, rmse, mape\n",
    "\n",
    "\n",
    "def get_lstm(data, train_len, test_len, lstm_len=4):\n",
    "    # prepare train and test data\n",
    "    data = data.tail(test_len + train_len).reset_index(drop=True)\n",
    "    dataset = np.reshape(data.values, (len(data), 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "\n",
    "    for i in range(lstm_len, train_len):\n",
    "        x_train.append(dataset_scaled[i - lstm_len:i, 0])\n",
    "        y_train.append(dataset_scaled[i, 0])\n",
    "    for i in range(train_len, len(dataset_scaled)):\n",
    "        x_test.append(dataset_scaled[i - lstm_len:i, 0])\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = np.array(x_test)\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    # Set up & fit LSTM RNN\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=lstm_len, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "    model.add(LSTM(units=int(lstm_len/2)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    early_stopping = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)\n",
    "    model.fit(x_train, y_train, epochs=500, batch_size=1, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "    # Generate predictions\n",
    "    prediction = model.predict(x_test)\n",
    "    prediction = scaler.inverse_transform(prediction).tolist()\n",
    "\n",
    "    output = []\n",
    "    for i in range(len(prediction)):\n",
    "        output.extend(prediction[i])\n",
    "    prediction = output\n",
    "\n",
    "    # Generate error data\n",
    "    mse = mean_squared_error(data.tail(len(prediction)).values, prediction)\n",
    "    rmse = mse ** 0.5\n",
    "    mape = mean_absolute_percentage_error(data.tail(len(prediction)).reset_index(drop=True), pd.Series(prediction))\n",
    "    return prediction, mse, rmse, mape\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load historical data\n",
    "    # CSV should have columns: ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    data = pd.read_csv('YOUR-DATA-HERE.csv', index_col=0, header=0).tail(1500).reset_index(drop=True)\n",
    "\n",
    "    # Initialize moving averages from Ta-Lib, store functions in dictionary\n",
    "    talib_moving_averages = ['SMA', 'EMA', 'WMA', 'DEMA', 'KAMA', 'MIDPOINT', 'MIDPRICE', 'T3', 'TEMA', 'TRIMA']\n",
    "    functions = {}\n",
    "    for ma in talib_moving_averages:\n",
    "        functions[ma] = abstract.Function(ma)\n",
    "\n",
    "    # Determine kurtosis \"K\" values for MA period 4-99\n",
    "    kurtosis_results = {'period': []}\n",
    "    for i in range(4, 100):\n",
    "        kurtosis_results['period'].append(i)\n",
    "        for ma in talib_moving_averages:\n",
    "            # Run moving average, remove last 252 days (used later for test data set), trim MA result to last 60 days\n",
    "            ma_output = functions[ma](data[:-252], i).tail(60)\n",
    "\n",
    "            # Determine kurtosis \"K\" value\n",
    "            k = kurtosis(ma_output, fisher=False)\n",
    "\n",
    "            # add to dictionary\n",
    "            if ma not in kurtosis_results.keys():\n",
    "                kurtosis_results[ma] = []\n",
    "            kurtosis_results[ma].append(k)\n",
    "\n",
    "    kurtosis_results = pd.DataFrame(kurtosis_results)\n",
    "    kurtosis_results.to_csv('kurtosis_results.csv')\n",
    "\n",
    "    # Determine period with K closest to 3 +/-5%\n",
    "    optimized_period = {}\n",
    "    for ma in talib_moving_averages:\n",
    "        difference = np.abs(kurtosis_results[ma] - 3)\n",
    "        df = pd.DataFrame({'difference': difference, 'period': kurtosis_results['period']})\n",
    "        df = df.sort_values(by=['difference'], ascending=True).reset_index(drop=True)\n",
    "        if df.at[0, 'difference'] < 3 * 0.05:\n",
    "            optimized_period[ma] = df.at[0, 'period']\n",
    "        else:\n",
    "            print(ma + ' is not viable, best K greater or less than 3 +/-5%')\n",
    "\n",
    "    print('\\nOptimized periods:', optimized_period)\n",
    "\n",
    "    simulation = {}\n",
    "    for ma in optimized_period:\n",
    "        # Split data into low volatility and high volatility time series\n",
    "        low_vol = functions[ma](data, optimized_period[ma])\n",
    "        high_vol = data['close'] - low_vol\n",
    "\n",
    "        # Generate ARIMA and LSTM predictions\n",
    "        print('\\nWorking on ' + ma + ' predictions')\n",
    "        try:\n",
    "            low_vol_prediction, low_vol_mse, low_vol_rmse, low_vol_mape = get_arima(low_vol, 1000, 252)\n",
    "        except:\n",
    "            print('ARIMA error, skipping to next MA type')\n",
    "            continue\n",
    "\n",
    "        high_vol_prediction, high_vol_mse, high_vol_rmse, high_vol_mape = get_lstm(high_vol, 1000, 252)\n",
    "\n",
    "        final_prediction = pd.Series(low_vol_prediction) + pd.Series(high_vol_prediction)\n",
    "        mse = mean_squared_error(final_prediction.values, data['close'].tail(252).values)\n",
    "        rmse = mse ** 0.5\n",
    "        mape = mean_absolute_percentage_error(data['close'].tail(252).reset_index(drop=True), final_prediction)\n",
    "\n",
    "        # Generate prediction accuracy\n",
    "        actual = data['close'].tail(252).values\n",
    "        result_1 = []\n",
    "        result_2 = []\n",
    "        for i in range(1, len(final_prediction)):\n",
    "            # Compare prediction to previous close price\n",
    "            if final_prediction[i] > actual[i-1] and actual[i] > actual[i-1]:\n",
    "                result_1.append(1)\n",
    "            elif final_prediction[i] < actual[i-1] and actual[i] < actual[i-1]:\n",
    "                result_1.append(1)\n",
    "            else:\n",
    "                result_1.append(0)\n",
    "\n",
    "            # Compare prediction to previous prediction\n",
    "            if final_prediction[i] > final_prediction[i-1] and actual[i] > actual[i-1]:\n",
    "                result_2.append(1)\n",
    "            elif final_prediction[i] < final_prediction[i-1] and actual[i] < actual[i-1]:\n",
    "                result_2.append(1)\n",
    "            else:\n",
    "                result_2.append(0)\n",
    "\n",
    "        accuracy_1 = np.mean(result_1)\n",
    "        accuracy_2 = np.mean(result_2)\n",
    "\n",
    "        simulation[ma] = {'low_vol': {'prediction': low_vol_prediction, 'mse': low_vol_mse,\n",
    "                                      'rmse': low_vol_rmse, 'mape': low_vol_mape},\n",
    "                          'high_vol': {'prediction': high_vol_prediction, 'mse': high_vol_mse,\n",
    "                                       'rmse': high_vol_rmse},\n",
    "                          'final': {'prediction': final_prediction.values.tolist(), 'mse': mse,\n",
    "                                    'rmse': rmse, 'mape': mape},\n",
    "                          'accuracy': {'prediction vs close': accuracy_1, 'prediction vs prediction': accuracy_2}}\n",
    "\n",
    "        # save simulation data here as checkpoint\n",
    "        with open('simulation_data.json', 'w') as fp:\n",
    "            json.dump(simulation, fp)\n",
    "\n",
    "    for ma in simulation.keys():\n",
    "        print('\\n' + ma)\n",
    "        print('Prediction vs Close:\\t\\t' + str(round(100*simulation[ma]['accuracy']['prediction vs close'], 2))\n",
    "              + '% Accuracy')\n",
    "        print('Prediction vs Prediction:\\t' + str(round(100*simulation[ma]['accuracy']['prediction vs prediction'], 2))\n",
    "              + '% Accuracy')\n",
    "        print('MSE:\\t', simulation[ma]['final']['mse'],\n",
    "              '\\nRMSE:\\t', simulation[ma]['final']['rmse'],\n",
    "              '\\nMAPE:\\t', simulation[ma]['final']['mape'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
